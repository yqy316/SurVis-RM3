const generatedBibEntries = {
    "asai2023self": {
        "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (SELF-RAG) that enhances an LM\u2019s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELFRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.1",
        "author": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi",
        "doi": "10.48550/arXiv.2310.11511",
        "journal": "arXiv preprint",
        "keywords": "type:Retrieval, LLM, RAG, Self-Reflection, arXiv, 2023, critique, verification",
        "number": "",
        "publisher": "ArXiv",
        "series": "ArXiv preprint",
        "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
        "type": "article",
        "url": "https://arxiv.org/abs/2310.11511",
        "volume": "",
        "year": "2023"
    },
    "devlin2019bert": {
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
        "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
        "doi": "10.48550/arXiv.1810.04805",
        "journal": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
        "keywords": "type:Transformer, NAACL, 2019, pre-training, language_representation, bidirectional",
        "number": "",
        "publisher": "Association for Computational Linguistics",
        "series": "NAACL",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "type": "inproceedings",
        "url": "https://arxiv.org/abs/1810.04805",
        "volume": "1",
        "year": "2019"
    },
    "gao2023retrieval": {
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domainspecific information. RAG synergistically merges LLMs\u2019 intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-theart technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development",
        "author": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang",
        "doi": "10.48550/arXiv.2312.10997",
        "journal": "arXiv preprint",
        "keywords": "type:Retrieval, LLM, RAG, Survey, arXiv, 2023, knowledge_augmentation, information_retrieval",
        "number": "",
        "publisher": "",
        "series": "",
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "type": "article",
        "url": "http://arxiv.org/abs/2312.10997",
        "volume": "",
        "year": "2023"
    },
    "lewis2020retrieval": {
        "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index ofWikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
        "author": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela",
        "doi": "10.48550/arXiv.2005.11401",
        "journal": "Advances in Neural Information Processing Systems",
        "keywords": "type:Retrieval, LLM, RAG, NeurIPS, 2020, knowledge_augmentation, information_retrieval",
        "number": "",
        "publisher": "Curran Associates Inc.",
        "series": "NeurIPS",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "type": "inproceedings",
        "url": "https://arxiv.org/abs/2005.11401",
        "volume": "33",
        "year": "2020"
    },
    "liu2021pre": {
        "abstract": "This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning\u201d. Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\ue030 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string \u02c6x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g. the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly-updated survey, and paperlist.",
        "author": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig",
        "doi": "10.48550/arXiv.2107.13586",
        "journal": "ACM Computing Surveys",
        "keywords": "type:Prompting, LLM, Survey, arXiv, 2021, pretrained_language_models, few-shot_learning",
        "number": "9",
        "publisher": "ACM",
        "series": "ACM Computing Surveys",
        "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
        "type": "article",
        "url": "https://arxiv.org/abs/2107.13586",
        "volume": "55",
        "year": "2021"
    },
    "nakano2021webgpt": {
        "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based webbrowsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model\u2019s answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
        "author": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman",
        "doi": "10.48550/arXiv.2112.09332",
        "journal": "arXiv preprint",
        "keywords": "type:LLM, Web, Human Feedback, arXiv, 2021, browser_interaction, RLHF",
        "number": "",
        "publisher": "",
        "series": "",
        "title": "WebGPT: Browser-assisted question-answering with human feedback",
        "type": "article",
        "url": "https://arxiv.org/abs/2112.09332",
        "volume": "",
        "year": "2021"
    },
    "shin2020autoprompt": {
        "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fillin-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AUTOPROMPT, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTOPROMPT, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning",
        "author": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, Sameer Singh",
        "doi": "10.48550/arXiv.2010.15980",
        "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
        "keywords": "type:Prompting, LLM, EMNLP, 2020, knowledge_probing, language_models",
        "number": "",
        "publisher": "Association for Computational Linguistics",
        "series": "EMNLP",
        "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "type": "inproceedings",
        "url": "http://arxiv.org/abs/2010.15980",
        "volume": "",
        "year": "2020"
    },
    "vaswani2017attention": {
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",
        "doi": "10.48550/arXiv.1706.03762",
        "journal": "Advances in Neural Information Processing Systems",
        "keywords": "type:Transformer, Attention, NeurIPS, 2017, machine_translation, neural_networks",
        "number": "",
        "publisher": "",
        "series": "NeurIPS",
        "title": "Attention Is All You Need",
        "type": "inproceedings",
        "url": "https://arxiv.org/abs/1706.03762",
        "volume": "30",
        "year": "2017"
    },
    "wei2022chain": {
        "abstract": "We explore how generating a chain ofthought\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier",
        "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou",
        "doi": "10.48550/arXiv.2201.11903",
        "journal": "Advances in Neural Information Processing Systems",
        "keywords": "type:Prompting, Reasoning, LLM, NeurIPS, 2022, emergent_abilities, complex_reasoning",
        "number": "",
        "publisher": "",
        "series": "NeurIPS",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "type": "inproceedings",
        "url": "https://arxiv.org/abs/2201.11903",
        "volume": "35",
        "year": "2022"
    },
    "yao2022react": {
        "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
        "author": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao",
        "doi": "10.48550/arXiv.2210.03629",
        "journal": "arXiv preprint",
        "keywords": "type:Reasoning, LLM, Acting, arXiv, 2022, tool_use, multi-step_tasks",
        "number": "",
        "publisher": "ArXiv",
        "series": "ArXiv preprint",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "type": "article",
        "url": "https://arxiv.org/abs/2210.03629",
        "volume": "",
        "year": "2022"
    }
};